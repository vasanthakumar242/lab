EX.NO:1
def mcp_neuron(inputs, weights, threshold): 
    summation = sum(i * w for i, w in zip(inputs, weights)) 
    return 1 if summation >= threshold else 0 
# AND Gate 
def AND(x1, x2): 
    return mcp_neuron([x1, x2], [1, 1], 2) 
# OR Gate 
def OR(x1, x2): 
    return mcp_neuron([x1, x2], [1, 1], 1) 
# NOT Gate 
def NOT(x1): 
    return mcp_neuron([x1], [-1], 0) 
# NOR Gate 
def NOR(x1, x2): 
    return mcp_neuron([x1, x2], [-1, -1], 0) 
# XOR Gate (using hard-coded logic) 
def XOR(x1, x2): 
    return (x1 ^ x2)  # XOR can't be represented by single-layer MCP 
# Testing 
print("AND") 
for x in [(0,0), (0,1), (1,0), (1,1)]: 
    print(f"{x} -> {AND(*x)}") 
print("\nOR") 
for x in [(0,0), (0,1), (1,0), (1,1)]: 
    print(f"{x} -> {OR(*x)}") 
print("\nNOT") 
for x in [0, 1]: 
    print(f"{x} -> {NOT(x)}") 
print("\nNOR") 
for x in [(0,0), (0,1), (1,0), (1,1)]: 
    print(f"{x} -> {NOR(*x)}") 
print("\nXOR") 
for x in [(0,0), (0,1), (1,0), (1,1)]: 
    print(f"{x} -> {XOR(*x)}") 
 

Ex. No. 2
Program: 
import numpy as np 
from sklearn.model_selection import train_test_split 
from sklearn.datasets import load_breast_cancer 
from sklearn.metrics import accuracy_score 
# 1. Load dataset (using breast cancer dataset for demonstration) 
X, y = load_breast_cancer(return_X_y=True) 
# Split the data 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 
# 2. Normalize the data (a simple standardization) 
# Avoid dividing by zero if std is 0 
X_train = (X_train - np.mean(X_train, axis=0)) / (np.std(X_train, axis=0) + 1e-8) 
X_test = (X_test - np.mean(X_test, axis=0)) / (np.std(X_test, axis=0) + 1e-8) 
class LogisticRegressionModel: 
    def __init__(self, learning_rate=0.01, num_iterations=1000): 
        self.learning_rate = learning_rate 
        self.num_iterations = num_iterations 
        self.weights = None 
        self.bias = None 
# 4. Implement forward propagation using sigmoid activation. 
    def _sigmoid(self, z): 
        return 1 / (1 + np.exp(-z)) 
    # 3. Initialize parameters (weights and bias). 
    def fit(self, X, y): 
        n_samples, n_features = X.shape 
        self.weights = np.zeros(n_features) 
        self.bias = 0 
        # 6. Perform gradient descent (backward propagation). 
        for iteration in range(self.num_iterations): 
            # Forward pass 
            linear_model = np.dot(X, self.weights) + self.bias 
            y_predicted = self._sigmoid(linear_model) 
            # 5. Compute cost (Binary Cross-Entropy) 
            # Adding a small value (1e-8) to prevent log(0) 
            cost = -np.mean(y * np.log(y_predicted + 1e-8) + (1 - y) * np.log(1 - y_predicted + 1e-8)) 
            # Backpropagation 
            dw = (1/n_samples) * np.dot(X.T, (y_predicted - y)) 
            db = (1/n_samples) * np.sum(y_predicted - y) 
            # Update parameters 
            self.weights -= self.learning_rate * dw 
            self.bias -= self.learning_rate * db 
            # Print cost every 100 iterations 
            if (iteration+1) % 100 == 0: 
                print(f"Iteration {iteration+1}/{self.num_iterations}, Cost: {cost:.4f}") 
    # 7. Predict and evaluate model performance. 
    def predict(self, X): 
        linear_model = np.dot(X, self.weights) + self.bias 
        y_predicted = self._sigmoid(linear_model) 
        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted] 
        return np.array(y_predicted_cls) 
# Instantiate and train the custom model 
lr_model = LogisticRegressionModel(learning_rate=0.001, num_iterations=5000) 
lr_model.fit(X_train, y_train) 
# Predict on the test set 
y_pred = lr_model.predict(X_test) 
# Evaluate performance 
accuracy = accuracy_score(y_test, y_pred) 
print("\nModel Accuracy on Test Set:", accuracy) 
 
  
 
 
Ex. No. 3 
Program: 
import numpy as np 
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import StandardScaler 
from sklearn.datasets import load_breast_cancer 
import matplotlib.pyplot as plt 
# Load dataset 
data = load_breast_cancer() 
X = data.data 
y = data.target.reshape(-1, 1) 
# Normalize 
scaler = StandardScaler() 
X = scaler.fit_transform(X) 
# Train-test split 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) 
# Initialize weights and biases 
input_dim = X.shape[1] 
hidden_dim = 10 
output_dim = 1 
W1 = np.random.randn(input_dim, hidden_dim) 
b1 = np.zeros((1, hidden_dim)) 
W2 = np.random.randn(hidden_dim, output_dim) 
b2 = np.zeros((1, output_dim)) 
def sigmoid(z): return 1 / (1 + np.exp(-z)) 
def tanh_derivative(a): return 1 - np.power(a, 2) 
epochs = 1000 
lr = 0.01 
losses = [] 
for epoch in range(epochs): 
    # Forward pass 
    Z1 = np.dot(X_train, W1) + b1 
    A1 = np.tanh(Z1) 
    Z2 = np.dot(A1, W2) + b2 
    A2 = sigmoid(Z2) 
    # Loss 
    loss = -np.mean(y_train * np.log(A2 + 1e-9) + (1 - y_train) * np.log(1 - A2 + 1e-9)) 
    losses.append(loss) 
    # Backward pass 
    dZ2 = A2 - y_train 
    dW2 = np.dot(A1.T, dZ2) 
    db2 = np.sum(dZ2, axis=0, keepdims=True) 
    dA1 = np.dot(dZ2, W2.T) 
    dZ1 = dA1 * tanh_derivative(A1) 
    dW1 = np.dot(X_train.T, dZ1) 
    db1 = np.sum(dZ1, axis=0) 
    # Update 
    W1 -= lr * dW1 
    b1 -= lr * db1 
    W2 -= lr * dW2 
    b2 -= lr * db2 
    if epoch % 100 == 0: 
        print(f"Epoch {epoch}, Loss: {loss:.4f}") 
# Prediction 
Z1_test = np.dot(X_test, W1) + b1 
A1_test = np.tanh(Z1_test) 
Z2_test = np.dot(A1_test, W2) + b2 
A2_test = sigmoid(Z2_test) 
y_pred = (A2_test > 0.5).astype(int) 
accuracy = np.mean(y_pred == y_test) * 100 
print(f"Test Accuracy: {accuracy:.2f}%") 
# Plot loss curve 
plt.plot(losses) 
plt.title("Loss Curve") 

EX.NO:4
Program:
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train.reshape(-1,28,28,1)/255.0
X_test = X_test.reshape(-1,28,28,1)/255.0
model = Sequential([
Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),
MaxPooling2D((2,2)),
Flatten(),
Dense(128, activation='relu'),
Dense(10, activation='softmax')
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))
plt.xlabel("Epochs") 
plt.ylabel("Loss") 
plt.show() 
 
